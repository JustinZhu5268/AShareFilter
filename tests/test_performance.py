#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ€§èƒ½æµ‹è¯•æ¨¡å—
åŠŸèƒ½ï¼š
1. å¤§æ•°æ®é‡ç­›é€‰æ€§èƒ½æµ‹è¯•
2. ç¼“å­˜æ•ˆç‡æµ‹è¯•
3. å¹¶å‘å¤„ç†æµ‹è¯•
4. å†…å­˜ä½¿ç”¨æµ‹è¯•
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import unittest
import time
import pandas as pd
import numpy as np
import datetime
import threading
import gc
from typing import List, Dict, Any


class TestPerformance(unittest.TestCase):
    """æ€§èƒ½æµ‹è¯•"""

    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.test_results = {}

    def test_large_dataset_filter_performance(self):
        """æµ‹è¯•å¤§æ•°æ®é‡ç­›é€‰æ€§èƒ½"""
        print("\nğŸ“Š å¤§æ•°æ®é‡ç­›é€‰æ€§èƒ½æµ‹è¯•...")

        # ç”Ÿæˆå¤§é‡æ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®
        num_stocks = 1000
        industries = ['ç”µæ°”è®¾å¤‡', 'å…ƒå™¨ä»¶', 'ä¸“ç”¨æœºæ¢°', 'è½¯ä»¶æœåŠ¡', 'æ±½è½¦é…ä»¶',
                     'åŠå¯¼ä½“', 'åŒ»è¯', 'é“¶è¡Œ', 'ç™½é…’', 'å®¶ç”µ']

        stocks = []
        for i in range(num_stocks):
            stocks.append({
                'ts_code': f'{600000+i}.SH',
                'symbol': f'{600000+i}',
                'name': f'è‚¡ç¥¨{i}',
                'industry': industries[i % len(industries)],
                'list_date': '20200101',
                'list_status': 'L',
            })

        df = pd.DataFrame(stocks)

        # æµ‹è¯•ç­›é€‰æ€§èƒ½
        start_time = time.time()

        # æ¨¡æ‹Ÿç­›é€‰é€»è¾‘
        for industry in industries[:5]:
            ind_stocks = df[df['industry'] == industry]
            valid_stocks = ind_stocks[ind_stocks['list_status'] == 'L']
            # æ¨¡æ‹Ÿå¸‚å€¼æ£€æŸ¥
            for _ in valid_stocks.head(10).itertuples():
                pass

        elapsed = time.time() - start_time

        print(f"   å¤„ç† {num_stocks} åªè‚¡ç¥¨è€—æ—¶: {elapsed:.3f}ç§’")

        # æ€§èƒ½æ–­è¨€
        self.assertLess(elapsed, 5.0, "å¤§æ•°æ®ç­›é€‰åº”åœ¨5ç§’å†…å®Œæˆ")
        self.test_results['large_dataset'] = elapsed

    def test_cache_efficiency(self):
        """æµ‹è¯•ç¼“å­˜æ•ˆç‡"""
        print("\nğŸ’¾ ç¼“å­˜æ•ˆç‡æµ‹è¯•...")

        from data.cache_manager import CACHE_DIR, ensure_cache_dir
        from data.mock_data import generate_mock_stock_list

        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        ensure_cache_dir()

        # æµ‹è¯•ç¼“å­˜è¯»å†™
        test_data = generate_mock_stock_list()

        # å†™å…¥æµ‹è¯•
        start_time = time.time()
        cache_file = CACHE_DIR / 'test_cache.csv'
        test_data.to_csv(cache_file, index=False, encoding='utf-8-sig')
        write_time = time.time() - start_time

        # è¯»å–æµ‹è¯•
        start_time = time.time()
        loaded_data = pd.read_csv(cache_file, encoding='utf-8-sig')
        read_time = time.time() - start_time

        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if cache_file.exists():
            cache_file.unlink()

        print(f"   å†™å…¥ {len(test_data)} æ¡æ•°æ®: {write_time*1000:.2f}ms")
        print(f"   è¯»å– {len(loaded_data)} æ¡æ•°æ®: {read_time*1000:.2f}ms")

        # ç¼“å­˜è¯»å†™åº”åœ¨åˆç†æ—¶é—´å†…å®Œæˆ
        self.assertLess(write_time, 1.0, "ç¼“å­˜å†™å…¥åº”åœ¨1ç§’å†…å®Œæˆ")
        self.assertLess(read_time, 1.0, "ç¼“å­˜è¯»å–åº”åœ¨1ç§’å†…å®Œæˆ")

        self.test_results['cache_write'] = write_time
        self.test_results['cache_read'] = read_time

    def test_concurrent_data_loading(self):
        """æµ‹è¯•å¹¶å‘æ•°æ®åŠ è½½"""
        print("\nğŸ”„ å¹¶å‘æ•°æ®åŠ è½½æµ‹è¯•...")

        from data.mock_data import generate_mock_daily_data

        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_stocks = ['300274.SZ', '000001.SZ', '600519.SH', '000858.SZ', '300750.SZ']

        results = {}
        errors = []

        def load_stock_data(ts_code):
            """åŠ è½½å•åªè‚¡ç¥¨æ•°æ®"""
            try:
                start = time.time()
                df = generate_mock_daily_data(ts_code, days=120)
                elapsed = time.time() - start
                results[ts_code] = elapsed
            except Exception as e:
                errors.append(str(e))

        # å¹¶å‘æ‰§è¡Œ
        threads = []
        start_time = time.time()

        for ts_code in test_stocks:
            thread = threading.Thread(target=load_stock_data, args=(ts_code,))
            threads.append(thread)
            thread.start()

        for thread in threads:
            thread.join()

        total_time = time.time() - start_time

        print(f"   å¹¶å‘åŠ è½½ {len(test_stocks)} åªè‚¡ç¥¨: {total_time:.3f}ç§’")
        print(f"   æˆåŠŸ: {len(results)}, å¤±è´¥: {len(errors)}")

        # å¹¶å‘åº”è¯¥æ­£å¸¸å·¥ä½œ
        self.assertEqual(len(errors), 0, f"å¹¶å‘åŠ è½½å‡ºé”™: {errors}")
        self.assertEqual(len(results), len(test_stocks), "æ‰€æœ‰è‚¡ç¥¨åº”åŠ è½½æˆåŠŸ")

        self.test_results['concurrent_time'] = total_time

    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        print("\nğŸ§  å†…å­˜ä½¿ç”¨æµ‹è¯•...")

        try:
            import psutil
            process = psutil.Process()

            # è·å–åˆå§‹å†…å­˜
            gc.collect()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB

            # ç”Ÿæˆå¤§é‡æ•°æ®
            large_data = []
            for i in range(100):
                df = pd.DataFrame({
                    'col1': range(1000),
                    'col2': range(1000),
                })
                large_data.append(df)

            # è·å–å³°å€¼å†…å­˜
            peak_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = peak_memory - initial_memory

            print(f"   åˆå§‹å†…å­˜: {initial_memory:.1f} MB")
            print(f"   å³°å€¼å†…å­˜: {peak_memory:.1f} MB")
            print(f"   å†…å­˜å¢é•¿: {memory_increase:.1f} MB")

            # æ¸…ç†
            del large_data
            gc.collect()

            self.test_results['memory_increase'] = memory_increase

        except ImportError:
            print("   âš ï¸ psutil æœªå®‰è£…ï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
            self.test_results['memory_increase'] = 0

    def test_api_response_time(self):
        """æµ‹è¯•APIå“åº”æ—¶é—´"""
        print("\nâ±ï¸ APIå“åº”æ—¶é—´æµ‹è¯•...")

        from data.mock_data import (
            generate_mock_stock_list,
            generate_mock_market_cap,
            generate_mock_financial_ttm,
        )

        # æµ‹è¯•å„APIå“åº”æ—¶é—´
        apis = {
            'stock_list': lambda: generate_mock_stock_list(),
            'market_cap': lambda: generate_mock_market_cap(),
            'financial_ttm': lambda: generate_mock_financial_ttm(),
        }

        api_times = {}
        for name, func in apis.items():
            start = time.time()
            result = func()
            elapsed = time.time() - start
            api_times[name] = elapsed * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            print(f"   {name}: {elapsed*1000:.2f}ms")

        # æ‰€æœ‰APIåº”åœ¨åˆç†æ—¶é—´å†…å“åº”
        for name, elapsed in api_times.items():
            self.assertLess(elapsed, 1000, f"{name} å“åº”æ—¶é—´è¶…è¿‡1ç§’")

        self.test_results['api_times'] = api_times

    def test_batch_processing_performance(self):
        """æµ‹è¯•æ‰¹é‡å¤„ç†æ€§èƒ½"""
        print("\nğŸ“¦ æ‰¹é‡å¤„ç†æ€§èƒ½æµ‹è¯•...")

        # æ¨¡æ‹Ÿæ‰¹é‡å¤„ç†ä»»åŠ¡
        batch_sizes = [10, 50, 100, 500]
        times = []

        for batch_size in batch_sizes:
            start_time = time.time()

            # æ¨¡æ‹Ÿæ‰¹é‡å¤„ç†
            results = []
            for i in range(batch_size):
                # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
                df = pd.DataFrame({'a': range(100), 'b': range(100)})
                result = df.mean().to_dict()
                results.append(result)

            elapsed = time.time() - start_time
            times.append(elapsed)
            print(f"   æ‰¹é‡ {batch_size}: {elapsed:.3f}ç§’")

        # æ‰¹é‡å¤„ç†åº”å‘ˆçº¿æ€§å¢é•¿
        # 500ä¸ªä»»åŠ¡åº”åœ¨10ç§’å†…å®Œæˆ
        self.assertLess(times[-1], 10.0, "æ‰¹é‡å¤„ç†500ä¸ªä»»åŠ¡åº”åœ¨10ç§’å†…å®Œæˆ")

        self.test_results['batch_times'] = dict(zip(batch_sizes, times))


class TestCacheConsistency(unittest.TestCase):
    """ç¼“å­˜ä¸€è‡´æ€§æµ‹è¯•"""

    def test_cache_data_integrity(self):
        """æµ‹è¯•ç¼“å­˜æ•°æ®å®Œæ•´æ€§"""
        print("\nğŸ”’ ç¼“å­˜æ•°æ®å®Œæ•´æ€§æµ‹è¯•...")

        from data.cache_manager import CACHE_DIR, ensure_cache_dir
        from data.mock_data import generate_mock_stock_list

        ensure_cache_dir()

        test_data = generate_mock_stock_list()
        cache_file = CACHE_DIR / 'integrity_test.csv'

        # å†™å…¥ç¼“å­˜
        test_data.to_csv(cache_file, index=False, encoding='utf-8-sig')

        # è¯»å–ç¼“å­˜
        loaded_data = pd.read_csv(cache_file, encoding='utf-8-sig')

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        self.assertEqual(len(test_data), len(loaded_data), "æ•°æ®è¡Œæ•°ä¸åŒ¹é…")
        self.assertEqual(list(test_data.columns), list(loaded_data.columns), "æ•°æ®åˆ—ä¸åŒ¹é…")

        # æ¸…ç†
        if cache_file.exists():
            cache_file.unlink()

        print("   âœ… ç¼“å­˜æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")

    def test_runtime_cache_thread_safety(self):
        """æµ‹è¯•è¿è¡Œæ—¶ç¼“å­˜çº¿ç¨‹å®‰å…¨"""
        print("\nğŸ§µ è¿è¡Œæ—¶ç¼“å­˜çº¿ç¨‹å®‰å…¨æµ‹è¯•...")

        from api.tushare_client import TushareClient

        client = TushareClient(use_mock=True)
        results = {}
        errors = []

        def get_stock_list():
            try:
                for _ in range(10):
                    client.get_stock_list()
                results['success'] = True
            except Exception as e:
                errors.append(str(e))

        # å¹¶å‘è®¿é—®
        threads = []
        for _ in range(5):
            thread = threading.Thread(target=get_stock_list)
            threads.append(thread)
            thread.start()

        for thread in threads:
            thread.join()

        self.assertEqual(len(errors), 0, f"å¹¶å‘è®¿é—®å‡ºé”™: {errors}")
        print("   âœ… çº¿ç¨‹å®‰å…¨éªŒè¯é€šè¿‡")


class TestDataProcessing(unittest.TestCase):
    """æ•°æ®å¤„ç†æµ‹è¯•"""

    def test_data_transformation_speed(self):
        """æµ‹è¯•æ•°æ®è½¬æ¢é€Ÿåº¦"""
        print("\nğŸ”„ æ•°æ®è½¬æ¢é€Ÿåº¦æµ‹è¯•...")

        # æ¨¡æ‹Ÿæ—¥çº¿æ•°æ®è½¬æ¢
        df = pd.DataFrame({
            'trade_date': pd.date_range('20240101', periods=1000),
            'open': np.random.randn(1000).cumsum() + 100,
            'high': np.random.randn(1000).cumsum() + 102,
            'low': np.random.randn(1000).cumsum() + 98,
            'close': np.random.randn(1000).cumsum() + 100,
            'vol': np.random.randint(100000, 1000000, 1000),
        })

        start = time.time()

        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        df['ma5'] = df['close'].rolling(5).mean()
        df['ma10'] = df['close'].rolling(10).mean()
        df['ma20'] = df['close'].rolling(20).mean()

        elapsed = time.time() - start

        print(f"   1000æ¡æ•°æ®æŒ‡æ ‡è®¡ç®—: {elapsed*1000:.2f}ms")

        self.assertLess(elapsed, 1.0, "æ•°æ®è½¬æ¢åº”åœ¨1ç§’å†…å®Œæˆ")

    def test_filter_chain_performance(self):
        """æµ‹è¯•ç­›é€‰é“¾æ€§èƒ½"""
        print("\nğŸ”— ç­›é€‰é“¾æ€§èƒ½æµ‹è¯•...")

        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        df = pd.DataFrame({
            'ts_code': [f'{600000+i}.SH' for i in range(100)],
            'industry': np.random.choice(['ç”µæ°”è®¾å¤‡', 'é“¶è¡Œ', 'ç™½é…’'], 100),
            'market_cap': np.random.uniform(10, 1000, 100),
            'roe': np.random.uniform(0, 30, 100),
        })

        start = time.time()

        # æ¨¡æ‹Ÿç­›é€‰é“¾
        result = df.copy()

        # Step 1: è¡Œä¸šç­›é€‰
        result = result[result['industry'].isin(['ç”µæ°”è®¾å¤‡', 'é“¶è¡Œ'])]

        # Step 2: å¸‚å€¼ç­›é€‰
        result = result[result['market_cap'] > 50]

        # Step 3: ROEç­›é€‰
        result = result[result['roe'] > 5]

        # Step 4: æ’åº
        result = result.sort_values('market_cap', ascending=False)

        elapsed = time.time() - start

        print(f"   ç­›é€‰é“¾æ‰§è¡Œ: {elapsed*1000:.2f}ms")
        print(f"   åŸå§‹: {len(df)} -> ç­›é€‰å: {len(result)}")

        self.assertLess(elapsed, 0.5, "ç­›é€‰é“¾åº”åœ¨0.5ç§’å†…å®Œæˆ")


class TestCacheHitRate(unittest.TestCase):
    """ç¼“å­˜å‘½ä¸­ç‡æµ‹è¯•"""

    def test_cache_hit_rate(self):
        """æµ‹è¯•ç¼“å­˜å‘½ä¸­ç‡"""
        print("\nğŸ¯ ç¼“å­˜å‘½ä¸­ç‡æµ‹è¯•...")

        from api.tushare_client import TushareClient
        from data.cache_manager import clear_all_cache

        # æ¸…ç©ºç¼“å­˜
        clear_all_cache()

        client = TushareClient(use_mock=True)

        # ç¬¬ä¸€æ¬¡è¯·æ±‚ - ç¼“å­˜æœªå‘½ä¸­
        start = time.time()
        result1 = client.get_stock_list()
        first_time = time.time() - start

        # ç¬¬äºŒæ¬¡è¯·æ±‚ - ç¼“å­˜å‘½ä¸­
        start = time.time()
        result2 = client.get_stock_list()
        second_time = time.time() - start

        # éªŒè¯ç¼“å­˜åŠ é€Ÿæ•ˆæœ
        print(f"   é¦–æ¬¡è¯·æ±‚: {first_time*1000:.2f}ms")
        print(f"   ç¼“å­˜å‘½ä¸­: {second_time*1000:.2f}ms")

        # ç¼“å­˜åº”è¯¥æ˜æ˜¾å¿«äºé¦–æ¬¡è¯·æ±‚
        # æ³¨æ„ï¼šç”±äº Mock æ•°æ®å¾ˆå¿«ï¼Œè¿™ä¸ªæµ‹è¯•å¯èƒ½ä¸æ˜¾è‘—
        self.assertIsNotNone(result1)
        self.assertIsNotNone(result2)
        print("âœ… ç¼“å­˜å‘½ä¸­ç‡æµ‹è¯•é€šè¿‡")

    def test_batch_request_performance(self):
        """æµ‹è¯•æ‰¹é‡è¯·æ±‚æ€§èƒ½"""
        print("\nğŸ“¦ æ‰¹é‡è¯·æ±‚æ€§èƒ½æµ‹è¯•...")

        from api.tushare_client import TushareClient

        client = TushareClient(use_mock=True)

        # æµ‹è¯•æ‰¹é‡è·å–å¸‚å€¼
        start = time.time()
        caps = client.get_all_market_caps()
        elapsed = time.time() - start

        print(f"   æ‰¹é‡è·å–å¸‚å€¼: {elapsed:.2f}ç§’")

        # éªŒè¯æ€§èƒ½æ»¡è¶³è¦æ±‚
        self.assertIsNotNone(caps)
        # Mock æ•°æ®åº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆ
        self.assertLess(elapsed, 30, "æ‰¹é‡è¯·æ±‚åº”åœ¨30ç§’å†…å®Œæˆ")
        print("âœ… æ‰¹é‡è¯·æ±‚æ€§èƒ½æµ‹è¯•é€šè¿‡")

    def test_concurrent_requests(self):
        """æµ‹è¯•å¹¶å‘è¯·æ±‚"""
        print("\nâš¡ å¹¶å‘è¯·æ±‚æµ‹è¯•...")

        import concurrent.futures
        from api.tushare_client import TushareClient

        client = TushareClient(use_mock=True)

        stock_codes = ['300274.SZ', '600519.SH', '000001.SZ', '000858.SZ',
                      '300750.SZ', '002594.SZ', '600036.SH', '000333.SZ']

        def fetch_stock(ts_code):
            return client.get_daily_data(ts_code, '20250101', '20250131')

        # å¹¶å‘è·å–å¤šåªè‚¡ç¥¨
        start = time.time()
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(fetch_stock, stock_codes))
        elapsed = time.time() - start

        print(f"   å¹¶å‘è¯·æ±‚ {len(stock_codes)} åªè‚¡ç¥¨: {elapsed:.2f}ç§’")

        # éªŒè¯æ‰€æœ‰è¯·æ±‚æˆåŠŸ
        self.assertEqual(len(results), len(stock_codes))
        print("âœ… å¹¶å‘è¯·æ±‚æµ‹è¯•é€šè¿‡")

    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        print("\nğŸ’¾ å†…å­˜ä½¿ç”¨æµ‹è¯•...")

        try:
            import tracemalloc
        except ImportError:
            print("   âš ï¸ tracemalloc ä¸å¯ç”¨ï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
            self.skipTest("tracemalloc not available")

        tracemalloc.start()

        # æ‰§è¡Œå¤§é‡æ•°æ®å¤„ç†
        from data.mock_data import generate_mock_daily_data

        for i in range(10):
            df = generate_mock_daily_data('300274.SZ', 120)

            # è®¡ç®—æŒ‡æ ‡
            df['ma5'] = df['close'].rolling(5).mean()
            df['ma10'] = df['close'].rolling(10).mean()
            df['ma20'] = df['close'].rolling(20).mean()

        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        peak_mb = peak / 1024 / 1024
        print(f"   å†…å­˜ä½¿ç”¨å³°å€¼: {peak_mb:.2f}MB")

        # éªŒè¯å†…å­˜ä½¿ç”¨åˆç†
        self.assertLess(peak_mb, 500, f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {peak_mb:.2f}MB")
        print("âœ… å†…å­˜ä½¿ç”¨æµ‹è¯•é€šè¿‡")


if __name__ == '__main__':
    unittest.main()
